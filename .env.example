# =============================================================================
# Claude-Mem Docker Configuration
# =============================================================================
# Copy this file to .env and configure your preferred AI provider.
#
# The worker service compresses observations into semantic memory.
# Choose ONE provider below based on your needs.
# =============================================================================

# =============================================================================
# AI PROVIDER SELECTION
# =============================================================================
# Options: ollama | openrouter | gemini | claude
#
# RECOMMENDED: ollama (fully local, no API costs)
# ALTERNATIVE: openrouter (free tier available, easy setup)

CLAUDE_MEM_PROVIDER=ollama

# =============================================================================
# OPTION 1: OLLAMA (Recommended - Fully Local)
# =============================================================================
# No API keys needed. Run your own models locally.
#
# Start with profile: docker compose --profile ollama up -d
# Or point to existing Ollama: CLAUDE_MEM_OLLAMA_URL=http://host.docker.internal:11434
#
# After starting, pull a model:
#   docker exec claude-mem-ollama ollama pull llama3.2:3b

CLAUDE_MEM_OLLAMA_URL=http://ollama:11434

# ---------------------------------------------------------------------------
# RECOMMENDED OLLAMA MODELS FOR OBSERVATION COMPRESSION
# ---------------------------------------------------------------------------
# Best balance of quality and speed:
#   llama3.2:3b      - Fast, good quality, 2GB VRAM (DEFAULT)
#   llama3.2:1b      - Fastest, lower quality, 1GB VRAM
#   qwen2.5:3b       - Excellent instruction following, 2GB VRAM
#   phi4:14b         - Best quality, requires 8GB+ VRAM
#   mistral:7b       - Great structured extraction, 4GB VRAM
#
# For GPU with 8GB+ VRAM:
#   llama3.3:70b-instruct-q4_K_M - Best quality (needs 40GB+ for full)
#   qwen2.5:14b      - Excellent quality
#   deepseek-r1:14b  - Strong reasoning

CLAUDE_MEM_OLLAMA_MODEL=llama3.2:3b

# ---------------------------------------------------------------------------
# OLLAMA EMBEDDING MODELS (for Chroma vector search)
# ---------------------------------------------------------------------------
# Pull separately: docker exec claude-mem-ollama ollama pull nomic-embed-text
#
# Best options:
#   nomic-embed-text     - Best general purpose (274MB)
#   mxbai-embed-large    - High quality (669MB)
#   all-minilm           - Fastest, smallest (46MB)
#   snowflake-arctic-embed - Strong retrieval performance

# CLAUDE_MEM_OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# =============================================================================
# OPTION 2: OPENROUTER (Remote API - Free Tier Available)
# =============================================================================
# Get API key at: https://openrouter.ai/keys
# Free tier: ~$0 with free models, no credit card required

# CLAUDE_MEM_OPENROUTER_API_KEY=sk-or-v1-...

# ---------------------------------------------------------------------------
# RECOMMENDED OPENROUTER MODELS
# ---------------------------------------------------------------------------
# FREE TIER (no cost):
#   google/gemini-2.0-flash-exp:free     - Best free model (RECOMMENDED)
#   meta-llama/llama-3.2-3b-instruct:free - Fast, reliable
#   mistralai/mistral-7b-instruct:free   - Good structured output
#   qwen/qwen-2.5-7b-instruct:free       - Strong instruction following
#
# PAID (low cost, better quality):
#   anthropic/claude-3.5-haiku           - $0.25/M input, fast
#   openai/gpt-4o-mini                   - $0.15/M input, reliable
#   google/gemini-flash-1.5-8b           - $0.0375/M input, very cheap
#   deepseek/deepseek-chat               - $0.14/M input, excellent value
#
# PREMIUM (best quality):
#   anthropic/claude-sonnet-4            - $3/M input
#   openai/gpt-4o                        - $2.50/M input
#   x-ai/grok-2                          - $2/M input
#   google/gemini-2.0-flash-thinking-exp - Free (experimental)

# CLAUDE_MEM_OPENROUTER_MODEL=google/gemini-2.0-flash-exp:free

# ---------------------------------------------------------------------------
# OPENAI VIA OPENROUTER
# ---------------------------------------------------------------------------
# Use OpenAI models without direct OpenAI account:
#   openai/gpt-4o-mini          - $0.15/M in, $0.60/M out
#   openai/gpt-4o               - $2.50/M in, $10/M out
#   openai/o1-mini              - $3/M in, $12/M out (reasoning)

# ---------------------------------------------------------------------------
# XAI (GROK) VIA OPENROUTER
# ---------------------------------------------------------------------------
# Elon's xAI models:
#   x-ai/grok-2                 - $2/M in, $10/M out
#   x-ai/grok-2-vision          - $2/M in, $10/M out (multimodal)
#   x-ai/grok-beta              - $5/M in, $15/M out

# ---------------------------------------------------------------------------
# MOONSHOT VIA OPENROUTER
# ---------------------------------------------------------------------------
# Chinese AI lab, competitive pricing:
#   moonshot/moonshot-v1-8k     - $0.14/M tokens
#   moonshot/moonshot-v1-32k    - $0.28/M tokens
#   moonshot/moonshot-v1-128k   - $0.84/M tokens (huge context)

# =============================================================================
# OPTION 3: GOOGLE GEMINI (Direct API)
# =============================================================================
# Get API key at: https://aistudio.google.com/apikey
# Free tier: 15 RPM, 1M tokens/day

# CLAUDE_MEM_GEMINI_API_KEY=...

# Models:
#   gemini-2.0-flash-exp       - Best (experimental, free)
#   gemini-1.5-flash           - Fast, reliable
#   gemini-1.5-flash-8b        - Fastest, cheapest
#   gemini-1.5-pro             - Highest quality

# CLAUDE_MEM_GEMINI_MODEL=gemini-2.0-flash-exp
# CLAUDE_MEM_GEMINI_RATE_LIMITING_ENABLED=true

# =============================================================================
# OPTION 4: ANTHROPIC CLAUDE (Direct API)
# =============================================================================
# Get API key at: https://console.anthropic.com/
# Note: Most expensive option, best for critical workloads

# ANTHROPIC_API_KEY=sk-ant-...
# CLAUDE_MEM_MODEL=claude-sonnet-4-5

# =============================================================================
# SERVICE CONFIGURATION
# =============================================================================

# Worker HTTP port
CLAUDE_MEM_WORKER_PORT=37777

# Log level: DEBUG | INFO | WARN | ERROR
CLAUDE_MEM_LOG_LEVEL=INFO

# =============================================================================
# ADVANCED: CONTEXT WINDOW MANAGEMENT
# =============================================================================
# For Ollama/OpenRouter - tune based on your model's context window

# Max messages to keep in conversation (sliding window)
# CLAUDE_MEM_OLLAMA_MAX_CONTEXT_MESSAGES=20

# Max estimated tokens before truncation
# CLAUDE_MEM_OLLAMA_MAX_TOKENS=100000

# =============================================================================
# QUICK START EXAMPLES
# =============================================================================
#
# Example 1: Fully local with Ollama
# -----------------------------------
# CLAUDE_MEM_PROVIDER=ollama
# CLAUDE_MEM_OLLAMA_MODEL=llama3.2:3b
# Then: docker compose --profile ollama up -d
#       docker exec claude-mem-ollama ollama pull llama3.2:3b
#
# Example 2: Free cloud with OpenRouter
# -------------------------------------
# CLAUDE_MEM_PROVIDER=openrouter
# CLAUDE_MEM_OPENROUTER_API_KEY=sk-or-v1-...
# CLAUDE_MEM_OPENROUTER_MODEL=google/gemini-2.0-flash-exp:free
# Then: docker compose up -d
#
# Example 3: OpenAI quality via OpenRouter
# ----------------------------------------
# CLAUDE_MEM_PROVIDER=openrouter
# CLAUDE_MEM_OPENROUTER_API_KEY=sk-or-v1-...
# CLAUDE_MEM_OPENROUTER_MODEL=openai/gpt-4o-mini
# Then: docker compose up -d
#
# =============================================================================
